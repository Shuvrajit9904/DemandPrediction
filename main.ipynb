{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing, model_selection, metrics\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file rows and columns are :  (1503424, 18)\n",
      "Test file rows and columns are :  (508438, 17)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/Users/shuvrajit/ComputerScience/Dev/KaggleChallanges/downloads/DemandPrediction/train.csv\", parse_dates=[\"activation_date\"])\n",
    "test_df = pd.read_csv(\"/Users/shuvrajit/ComputerScience/Dev/KaggleChallanges/downloads/DemandPrediction/test.csv\", parse_dates=[\"activation_date\"])\n",
    "print(\"Train file rows and columns are : \", train_df.shape)\n",
    "print(\"Test file rows and columns are : \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df[\"deal_probability\"].values\n",
    "test_id = test_df[\"item_id\"].values\n",
    "\n",
    "# New variable on weekday #\n",
    "train_df[\"activation_weekday\"] = train_df[\"activation_date\"].dt.weekday\n",
    "test_df[\"activation_weekday\"] = test_df[\"activation_date\"].dt.weekday\n",
    "\n",
    "# Label encode the categorical variables #\n",
    "cat_vars = [\"region\", \"city\", \"parent_category_name\", \"category_name\", \"user_type\", \"param_1\", \"param_2\", \"param_3\"]\n",
    "for col in cat_vars:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n",
    "    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n",
    "    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"item_id\", \"user_id\", \"title\", \"activation_date\", \"image\"]\n",
    "train_X = train_df.drop(cols_to_drop + [\"deal_probability\"], axis=1)\n",
    "test_X = test_df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Word Vector for Description\n",
    "EMBEDDING_FILE = f'/Users/shuvrajit/ComputerScience/Dev/KaggleChallanges/downloads/DemandPrediction/wiki.ru.vec'\n",
    "embed_size = 300\n",
    "max_features = 20000\n",
    "maxlen = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train_X['description'].fillna('_na_').values\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "#embeddings_index = dict(get_coefs(o.strip().split()[0], *o.strip().split()[1:]) for o in open(EMBEDDING_FILE))\n",
    "embedding_index = {}\n",
    "for o in open(EMBEDDING_FILE):\n",
    "    vec = o.strip().split(' ')\n",
    "    word = vec[0]\n",
    "    arr = np.asarray(vec[1:], dtype='float32')\n",
    "    if len(vec[1:]) == 300:\n",
    "        embedding_index[word] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for key in embedding_index:\n",
    "    i += 1\n",
    "    if len(embedding_index[key]) != 300:\n",
    "        print(key)\n",
    "        break\n",
    "#print(i, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.003528327, 0.2954682)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embedding_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(300, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(300, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"sigmoid\", name = 'my_layer')(x)\n",
    "#layer_name = 'my_layer'\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output = intermediate_layer_model.predict(X_train[1:5])\n",
    "#X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5145814 , 0.45168105, 0.44174707, 0.53815275, 0.49735138,\n",
       "        0.5191435 , 0.438084  , 0.49662465, 0.43768388, 0.45061246,\n",
       "        0.4574356 , 0.4744294 , 0.4641357 , 0.53288555, 0.5424755 ,\n",
       "        0.43678963, 0.4772676 , 0.532655  , 0.55526876, 0.48597708,\n",
       "        0.44658732, 0.50368047, 0.4931569 , 0.5274568 , 0.51022637,\n",
       "        0.49541217, 0.5171523 , 0.42435357, 0.50572634, 0.47048974,\n",
       "        0.45545664, 0.48040065, 0.49365968, 0.4708676 , 0.5717914 ,\n",
       "        0.49891979, 0.48603195, 0.43684295, 0.50723636, 0.5246791 ,\n",
       "        0.49917704, 0.4746427 , 0.4772224 , 0.52373767, 0.50451505,\n",
       "        0.5182245 , 0.46885806, 0.52016264, 0.5173303 , 0.48977652],\n",
       "       [0.48377913, 0.4701091 , 0.44298217, 0.53482664, 0.51578003,\n",
       "        0.5222676 , 0.44861495, 0.49661312, 0.44453806, 0.46103522,\n",
       "        0.4466633 , 0.46372524, 0.4509338 , 0.52669954, 0.53323054,\n",
       "        0.43376154, 0.46445298, 0.5430966 , 0.5510078 , 0.49908578,\n",
       "        0.45838836, 0.47341922, 0.51408046, 0.52581304, 0.5262017 ,\n",
       "        0.4885124 , 0.54143   , 0.4691799 , 0.5224021 , 0.47573248,\n",
       "        0.4414843 , 0.47561228, 0.5171438 , 0.46093774, 0.57444334,\n",
       "        0.50002277, 0.49684903, 0.45741567, 0.5090559 , 0.5120735 ,\n",
       "        0.5307193 , 0.4814472 , 0.48999372, 0.52214223, 0.48978835,\n",
       "        0.50629574, 0.4597536 , 0.51968384, 0.5443717 , 0.49000126],\n",
       "       [0.51561004, 0.46609122, 0.4577309 , 0.52971435, 0.4806655 ,\n",
       "        0.5230086 , 0.44498628, 0.5025944 , 0.4390144 , 0.46908984,\n",
       "        0.46102422, 0.4756453 , 0.46898457, 0.52832973, 0.5472284 ,\n",
       "        0.4584544 , 0.46663728, 0.52073705, 0.5511932 , 0.50742394,\n",
       "        0.452292  , 0.51823545, 0.5095388 , 0.51570755, 0.5173752 ,\n",
       "        0.4982078 , 0.5275847 , 0.45281646, 0.4952023 , 0.48104176,\n",
       "        0.46136212, 0.46697333, 0.48624396, 0.47013533, 0.5563506 ,\n",
       "        0.49612474, 0.48574385, 0.44963402, 0.48876777, 0.5102226 ,\n",
       "        0.52709514, 0.48007864, 0.49207506, 0.51975644, 0.50848144,\n",
       "        0.5103796 , 0.4727666 , 0.51410335, 0.53533137, 0.518384  ],\n",
       "       [0.51974714, 0.4336512 , 0.46532395, 0.531771  , 0.47704342,\n",
       "        0.52226245, 0.46663445, 0.49474353, 0.42106646, 0.4554807 ,\n",
       "        0.45183882, 0.47554526, 0.46534595, 0.5447228 , 0.55007833,\n",
       "        0.448006  , 0.480717  , 0.5347249 , 0.560426  , 0.50427634,\n",
       "        0.43042523, 0.5030709 , 0.5138103 , 0.49714017, 0.50404507,\n",
       "        0.4998633 , 0.54672146, 0.45337608, 0.49780264, 0.4927666 ,\n",
       "        0.44984746, 0.4726543 , 0.49672562, 0.45668632, 0.56079423,\n",
       "        0.50185436, 0.500474  , 0.44354215, 0.5048303 , 0.5052812 ,\n",
       "        0.52069116, 0.47467357, 0.49346843, 0.5441492 , 0.48758402,\n",
       "        0.5283867 , 0.46588087, 0.51196253, 0.52701944, 0.49427927]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2400/2400 [==============================] - 417s 174ms/step - loss: 0.0652\n",
      "Epoch 2/2\n",
      "2400/2400 [==============================] - 419s 175ms/step - loss: 0.0570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ad88a2e10>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train[:2400], train_y[:2400], epochs = 2, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_train[3001:4600], train_y[3001:4600], batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"num_leaves\" : 30,\n",
    "        \"learning_rate\" : 0.1,\n",
    "        \"bagging_fraction\" : 0.7,\n",
    "        \"feature_fraction\" : 0.7,\n",
    "        \"bagging_frequency\" : 5,\n",
    "        \"bagging_seed\" : 2018,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=20, evals_result=evals_result)\n",
    "    \n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    return pred_test_y, model, evals_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1303424, 12) (200000, 12) (508438, 12)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[20]\tvalid_0's rmse: 0.236221\n",
      "[40]\tvalid_0's rmse: 0.233507\n",
      "[60]\tvalid_0's rmse: 0.232136\n",
      "[80]\tvalid_0's rmse: 0.231493\n",
      "[100]\tvalid_0's rmse: 0.231028\n",
      "[120]\tvalid_0's rmse: 0.230668\n",
      "[140]\tvalid_0's rmse: 0.230338\n",
      "[160]\tvalid_0's rmse: 0.230086\n",
      "[180]\tvalid_0's rmse: 0.229867\n",
      "[200]\tvalid_0's rmse: 0.22968\n",
      "[220]\tvalid_0's rmse: 0.229557\n",
      "[240]\tvalid_0's rmse: 0.2294\n",
      "[260]\tvalid_0's rmse: 0.229287\n",
      "[280]\tvalid_0's rmse: 0.229155\n",
      "[300]\tvalid_0's rmse: 0.229047\n",
      "[320]\tvalid_0's rmse: 0.228926\n",
      "[340]\tvalid_0's rmse: 0.228818\n",
      "[360]\tvalid_0's rmse: 0.228743\n",
      "[380]\tvalid_0's rmse: 0.228671\n",
      "[400]\tvalid_0's rmse: 0.22859\n",
      "[420]\tvalid_0's rmse: 0.228505\n",
      "[440]\tvalid_0's rmse: 0.228418\n",
      "[460]\tvalid_0's rmse: 0.228355\n",
      "[480]\tvalid_0's rmse: 0.228291\n",
      "[500]\tvalid_0's rmse: 0.228261\n",
      "[520]\tvalid_0's rmse: 0.22821\n",
      "[540]\tvalid_0's rmse: 0.22815\n",
      "[560]\tvalid_0's rmse: 0.228073\n",
      "[580]\tvalid_0's rmse: 0.228022\n",
      "[600]\tvalid_0's rmse: 0.227969\n",
      "[620]\tvalid_0's rmse: 0.227926\n",
      "[640]\tvalid_0's rmse: 0.227887\n",
      "[660]\tvalid_0's rmse: 0.227844\n",
      "[680]\tvalid_0's rmse: 0.227804\n",
      "[700]\tvalid_0's rmse: 0.227766\n",
      "[720]\tvalid_0's rmse: 0.227738\n",
      "[740]\tvalid_0's rmse: 0.22771\n",
      "[760]\tvalid_0's rmse: 0.227664\n",
      "[780]\tvalid_0's rmse: 0.227633\n",
      "[800]\tvalid_0's rmse: 0.227592\n",
      "[820]\tvalid_0's rmse: 0.22755\n",
      "[840]\tvalid_0's rmse: 0.227526\n",
      "[860]\tvalid_0's rmse: 0.227489\n",
      "[880]\tvalid_0's rmse: 0.227456\n",
      "[900]\tvalid_0's rmse: 0.227423\n",
      "[920]\tvalid_0's rmse: 0.227385\n",
      "[940]\tvalid_0's rmse: 0.227356\n",
      "[960]\tvalid_0's rmse: 0.22732\n",
      "[980]\tvalid_0's rmse: 0.227303\n",
      "[1000]\tvalid_0's rmse: 0.227271\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 0.227271\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data for model training#\n",
    "dev_X = train_X.iloc[:-200000,:]\n",
    "val_X = train_X.iloc[-200000:,:]\n",
    "dev_y = train_y[:-200000]\n",
    "val_y = train_y[-200000:]\n",
    "print(dev_X.shape, val_X.shape, test_X.shape)\n",
    "\n",
    "# Training the model #\n",
    "pred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_X)\n",
    "\n",
    "# Making a submission file #\n",
    "pred_test[pred_test>1] = 1\n",
    "pred_test[pred_test<0] = 0\n",
    "sub_df = pd.DataFrame({\"item_id\":test_id})\n",
    "sub_df[\"deal_probability\"] = pred_test\n",
    "sub_df.to_csv(\"baseline_lgb.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
